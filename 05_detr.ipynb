{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4270c045",
   "metadata": {},
   "source": [
    "# Exercise 5: DETR Walkthrough\n",
    "\n",
    "Explore query-based object detection with DETR\n",
    "\n",
    "\n",
    "- a) Model inspection: Load DETR and identify components\n",
    "- b) Forward pass: Inspect output tensor shapes\n",
    "- c) No-object analysis: Count sparse predictions\n",
    "- d) Visualization: Post-process and display detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96705f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6bfc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load model and processor\n",
    "model_id = \"facebook/detr-resnet-50\"\n",
    "model = DetrForObjectDetection.from_pretrained(model_id)\n",
    "processor = DetrImageProcessor.from_pretrained(model_id)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cpu\")  # laptop-friendly\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec02fc",
   "metadata": {},
   "source": [
    "## Part a) Model Inspection\n",
    "\n",
    "Identify the main components of DETR:\n",
    "- CNN backbone (ResNet-50)\n",
    "- Transformer encoder\n",
    "- Transformer decoder\n",
    "- Prediction heads (class and box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c9a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access internal components\n",
    "print(\"DETR components\")\n",
    "print(f\"Backbone:\")\n",
    "model.model.backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b257b-61d2-48ab-baf5-54c94514d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Encoder:\")\n",
    "model.model.encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0de18-766d-43ea-9511-014994c052b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Decoder:\")\n",
    "model.model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object queries - the learnable embeddings\n",
    "query_embeds = model.model.query_position_embeddings.weight\n",
    "print(f\"Object Queries\")\n",
    "print(f\"Shape: {query_embeds.shape}\")\n",
    "print(f\"Number of queries (Q): {query_embeds.shape[0]}\")\n",
    "print(f\"Query dimension: {query_embeds.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction heads\n",
    "print(\"Prediction Heads\")\n",
    "print(f\"Class head: {model.class_labels_classifier}\")\n",
    "print(f\"Box head: {model.bbox_predictor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fadef6",
   "metadata": {},
   "source": [
    "## Part b) Forward Pass and Tensor Interpretation\n",
    "\n",
    "Run inference and inspect output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc871a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "url = \"https://cdn.mos.cms.futurecdn.net/vhQreQN76LUVdycsEDUFTH-1024-80.jpg\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "print(f\"Image size (W, H): {image.size}\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Input image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "print(f\"Input keys: {inputs.keys()}\")\n",
    "print(f\"pixel_values shape: {inputs['pixel_values'].shape}\")  # (B, 3, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(f\"Output shapes\")\n",
    "print(f\"logits: {outputs.logits.shape}\")          # (B, Q, K+1)\n",
    "print(f\"pred_boxes: {outputs.pred_boxes.shape}\")  # (B, Q, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why K+1 classes?\n",
    "num_classes = model.config.num_labels\n",
    "print(f\"Number of object classes: {num_classes}\")\n",
    "print(f\"Total classes in logits: {outputs.logits.shape[-1]}\")\n",
    "print(f\"Extra class is: no-object (background)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0171019",
   "metadata": {},
   "source": [
    "## Part c) No-Object Analysis\n",
    "\n",
    "Most queries predict \"no object\" - examine this sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584487d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits[0].cpu().numpy()  # (Q, K+1)\n",
    "pred_classes = np.argmax(logits, axis=-1)\n",
    "\n",
    "# In DETR, the last class index is \"no-object\"\n",
    "no_obj_id = model.config.num_labels  # 91 (index of no-object class)\n",
    "num_noobj = np.sum(pred_classes == no_obj_id)\n",
    "\n",
    "print(f\"Total queries: {logits.shape[0]}\")\n",
    "print(f\"Queries predicting 'no-object': {num_noobj}\")\n",
    "print(f\"Queries predicting actual objects: {logits.shape[0] - num_noobj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine confidence distribution\n",
    "# Apply softmax to get probabilities\n",
    "probs = torch.softmax(outputs.logits[0], dim=-1).cpu().numpy()\n",
    "max_probs = np.max(probs, axis=-1)  # max probability per query\n",
    "\n",
    "print(f\"\\nConfidence distribution:\")\n",
    "print(f\"  Max confidence: {max_probs.max():.3f}\")\n",
    "print(f\"  Min confidence: {max_probs.min():.3f}\")\n",
    "print(f\"  Mean confidence: {max_probs.mean():.3f}\")\n",
    "\n",
    "# Queries with high confidence for actual objects\n",
    "threshold = 0.7\n",
    "confident_objects = np.sum((pred_classes != no_obj_id) & (max_probs > threshold))\n",
    "print(f\"\\nQueries with object class and confidence > {threshold}: {confident_objects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b58c9d",
   "metadata": {},
   "source": [
    "## Part d) Post-processing and Visualization\n",
    "\n",
    "Convert normalized boxes to pixel coordinates and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76de89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process with threshold\n",
    "threshold = 0.7\n",
    "target_sizes = torch.tensor([image.size[::-1]], device=device)  # (H, W)\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs, target_sizes=target_sizes, threshold=threshold\n",
    ")[0]\n",
    "\n",
    "print(f\"Detections (threshold={threshold})\")\n",
    "print(f\"Number of detections: {len(results['boxes'])}\")\n",
    "print(f\"Boxes shape: {results['boxes'].shape}\")\n",
    "print(f\"Scores shape: {results['scores'].shape}\")\n",
    "print(f\"Labels shape: {results['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detections\n",
    "print(\"Detected objects:\")\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    cls = model.config.id2label[label.item()]\n",
    "    box_coords = [round(x, 1) for x in box.tolist()]\n",
    "    print(f\"  {cls:>12s}  score={score:.3f}  box(xyxy)={box_coords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ffdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detections\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.imshow(image)\n",
    "\n",
    "colors = plt.cm.tab10.colors\n",
    "\n",
    "for i, (score, label, box) in enumerate(zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])):\n",
    "    xmin, ymin, xmax, ymax = box.tolist()\n",
    "    w, h = xmax - xmin, ymax - ymin\n",
    "\n",
    "    color = colors[label.item() % len(colors)]\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin, ymin), w, h,\n",
    "        fill=False, linewidth=2, edgecolor=color\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    cls = model.config.id2label[label.item()]\n",
    "    ax.text(\n",
    "        xmin, ymin - 5,\n",
    "        f\"{cls} {score:.2f}\",\n",
    "        bbox=dict(facecolor=color, alpha=0.7),\n",
    "        fontsize=10, color=\"white\"\n",
    "    )\n",
    "\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(f\"DETR detections (threshold={threshold})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5fb376",
   "metadata": {},
   "source": [
    "## Effect of Threshold\n",
    "\n",
    "Lowering the threshold reveals more (potentially false) detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d58599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different thresholds\n",
    "thresholds = [0.9, 0.7, 0.5, 0.3]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(thresholds), figsize=(16, 4))\n",
    "\n",
    "for ax, thresh in zip(axes, thresholds):\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs, target_sizes=target_sizes, threshold=thresh\n",
    "    )[0]\n",
    "\n",
    "    ax.imshow(image)\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        xmin, ymin, xmax, ymax = box.tolist()\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin), xmax-xmin, ymax-ymin,\n",
    "            fill=False, linewidth=2, edgecolor=\"red\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.set_title(f\"threshold={thresh}\\n({len(results['boxes'])} detections)\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
